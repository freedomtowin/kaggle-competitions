{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit,njit, prange, types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 25)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'nom_0', 'nom_1',\n",
       "       'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9',\n",
       "       'ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5', 'day', 'month',\n",
       "       'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 24)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        600000\n",
       "bin_0          3\n",
       "bin_1          3\n",
       "bin_2          3\n",
       "bin_3          3\n",
       "bin_4          3\n",
       "nom_0          4\n",
       "nom_1          7\n",
       "nom_2          7\n",
       "nom_3          7\n",
       "nom_4          5\n",
       "nom_5       1221\n",
       "nom_6       1520\n",
       "nom_7        223\n",
       "nom_8        223\n",
       "nom_9       2219\n",
       "ord_0          4\n",
       "ord_1          6\n",
       "ord_2          7\n",
       "ord_3         16\n",
       "ord_4         27\n",
       "ord_5        191\n",
       "day            8\n",
       "month         13\n",
       "target         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.nunique(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       400000\n",
       "bin_0         3\n",
       "bin_1         3\n",
       "bin_2         3\n",
       "bin_3         3\n",
       "bin_4         3\n",
       "nom_0         4\n",
       "nom_1         7\n",
       "nom_2         7\n",
       "nom_3         7\n",
       "nom_4         5\n",
       "nom_5      1220\n",
       "nom_6      1518\n",
       "nom_7       223\n",
       "nom_8       223\n",
       "nom_9      2217\n",
       "ord_0         4\n",
       "ord_1         6\n",
       "ord_2         7\n",
       "ord_3        16\n",
       "ord_4        27\n",
       "ord_5       191\n",
       "day           8\n",
       "month        13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.nunique(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "bin_0     17894\n",
       "bin_1     18003\n",
       "bin_2     17930\n",
       "bin_3     18014\n",
       "bin_4     18047\n",
       "nom_0     18252\n",
       "nom_1     18156\n",
       "nom_2     18035\n",
       "nom_3     18121\n",
       "nom_4     18035\n",
       "nom_5     17778\n",
       "nom_6     18131\n",
       "nom_7     18003\n",
       "nom_8     17755\n",
       "nom_9     18073\n",
       "ord_0     18288\n",
       "ord_1     18041\n",
       "ord_2     18075\n",
       "ord_3     17916\n",
       "ord_4     17930\n",
       "ord_5     17713\n",
       "day       17952\n",
       "month     17988\n",
       "target        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "bin_0    11901\n",
       "bin_1    12038\n",
       "bin_2    11972\n",
       "bin_3    11951\n",
       "bin_4    11951\n",
       "nom_0    12062\n",
       "nom_1    11947\n",
       "nom_2    12179\n",
       "nom_3    12176\n",
       "nom_4    11993\n",
       "nom_5    11912\n",
       "nom_6    12012\n",
       "nom_7    12003\n",
       "nom_8    11956\n",
       "nom_9    12060\n",
       "ord_0    11893\n",
       "ord_1    12167\n",
       "ord_2    12105\n",
       "ord_3    12053\n",
       "ord_4    11933\n",
       "ord_5    12047\n",
       "day      12025\n",
       "month    11984\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['isnan']=train.isnull().sum(axis=1)\n",
    "test['isnan']=test.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna('nan')\n",
    "test = test.fillna('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\ndef split_0(x):\\n    x = str(x)\\n    if x=='nan':\\n        return x\\n    return x[:3]\\n\\ndef split_1(x):\\n    x = str(x)\\n    if len(x)<6:\\n        return 'nan'\\n    return x[3:6]\\n\\ndef split_2(x):\\n    x = str(x)\\n    if len(x)<9:\\n        return 'nan'\\n    return x[6:9]\\n    \\nnom_features = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\\nprint(len(np.unique(train[nom_features].values.astype(str).flatten())))\\n\\nuniq = []\\nnom_nom = []\\nnom_features = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\\nfor feature in nom_features:\\n\\n    train[feature+'_0']=train[feature].apply(lambda x: split_0(x))\\n    train[feature+'_1']=train[feature].apply(lambda x: split_1(x))\\n    train[feature+'_2']=train[feature].apply(lambda x: split_2(x))\\n    \\n    \\n    test[feature+'_0']=test[feature].apply(lambda x: split_0(x))\\n    test[feature+'_1']=test[feature].apply(lambda x: split_1(x))\\n    test[feature+'_2']=test[feature].apply(lambda x: split_2(x))\\n    \\n    nom_nom.append(feature+'_0')\\n    nom_nom.append(feature+'_1')\\n    nom_nom.append(feature+'_2')\\n    \\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "def split_0(x):\n",
    "    x = str(x)\n",
    "    if x=='nan':\n",
    "        return x\n",
    "    return x[:3]\n",
    "\n",
    "def split_1(x):\n",
    "    x = str(x)\n",
    "    if len(x)<6:\n",
    "        return 'nan'\n",
    "    return x[3:6]\n",
    "\n",
    "def split_2(x):\n",
    "    x = str(x)\n",
    "    if len(x)<9:\n",
    "        return 'nan'\n",
    "    return x[6:9]\n",
    "    \n",
    "nom_features = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n",
    "print(len(np.unique(train[nom_features].values.astype(str).flatten())))\n",
    "\n",
    "uniq = []\n",
    "nom_nom = []\n",
    "nom_features = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n",
    "for feature in nom_features:\n",
    "\n",
    "    train[feature+'_0']=train[feature].apply(lambda x: split_0(x))\n",
    "    train[feature+'_1']=train[feature].apply(lambda x: split_1(x))\n",
    "    train[feature+'_2']=train[feature].apply(lambda x: split_2(x))\n",
    "    \n",
    "    \n",
    "    test[feature+'_0']=test[feature].apply(lambda x: split_0(x))\n",
    "    test[feature+'_1']=test[feature].apply(lambda x: split_1(x))\n",
    "    test[feature+'_2']=test[feature].apply(lambda x: split_2(x))\n",
    "    \n",
    "    nom_nom.append(feature+'_0')\n",
    "    nom_nom.append(feature+'_1')\n",
    "    nom_nom.append(feature+'_2')\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing numpy on engine(s)\n",
      "importing gc on engine(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AsyncResult: execute:finished>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parallel processing will be used to apply a mean-value encoding rank transform to features in the pandas dataframe.\n",
    "\n",
    "This is accomplished by parallelizing the dataframe across the cores and applying map-reduce to transform the columns.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy\n",
    "import ipyparallel as ipp\n",
    "from ipyparallel import Client\n",
    "rc = Client(profile='default')\n",
    "dview = rc[:]\n",
    "dview.block = True\n",
    "\n",
    "\n",
    "# rc.debug = True\n",
    "with dview.sync_imports():\n",
    "    import numpy\n",
    "    import gc\n",
    "    \n",
    "    \n",
    "dview.execute('np=numpy')\n",
    "dview.execute('np.mkl.set_num_threads(2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = dview.targets\n",
    "\n",
    "num_cores = len(ids)\n",
    "\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_df(df):\n",
    "    df_split = np.array_split(df, num_cores, axis=0)\n",
    "\n",
    "    for i in dview.targets:\n",
    "        rc[i]['df'] = df_split[i]\n",
    "\n",
    "    rdf = [ipp.Reference('df') for i in range(num_cores)]\n",
    "    return rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_category_rank(df):\n",
    "    return df[[feature,'target']].groupby(feature).agg(['sum','count'])    \n",
    "\n",
    "def reduce_category_rank(x):\n",
    "    y = pd.concat(x).groupby(level=0).sum()\n",
    "    y.columns = y.columns.droplevel()\n",
    "    y['mean'] = y['sum']/y['count']\n",
    "    return y[['mean']]\n",
    "\n",
    "def map_unique(df):\n",
    "    return df[feature].unique().astype(str)\n",
    "\n",
    "def reduce_unique(x):\n",
    "    return np.unique(np.concatenate(x))\n",
    "\n",
    "def map_missing_rank_transform(df):\n",
    "    df[feature] = df[feature].astype(str).replace(new_category_mapper)\n",
    "    \n",
    "def map_rank_transform(df):\n",
    "#     for key,value in mappers[feature].items():\n",
    "    df[feature] = df[feature].astype(str).replace(mappers[feature])\n",
    "    \n",
    "    \n",
    "class RankTransformParallel:\n",
    "    def __init__( self,dview,ordinal=True):\n",
    "        self.features = []\n",
    "        self.mappers = {}\n",
    "        self.dview = dview\n",
    "        self.ids = dview.targets\n",
    "        self.ordinal=ordinal\n",
    "        \n",
    "    \n",
    "    def fit( self, rdf, features, target ):\n",
    "        self.features = features\n",
    "\n",
    "        for feature in self.features:\n",
    "            self.dview['feature'] = feature\n",
    "            _mapper_ = {}\n",
    "            result=self.dview.map_sync(map_category_rank,rdf)\n",
    "            result=reduce_category_rank(result)\n",
    "\n",
    "            names = result.index.values.astype(str)\n",
    "            \n",
    "            if self.ordinal==True:\n",
    "                values = np.argsort(result['mean'].values)\n",
    "            else:\n",
    "                values = result['mean'].values\n",
    "\n",
    "            for n,v in zip(names,values):\n",
    "                _mapper_[n] = v\n",
    "                \n",
    "            self.mappers[feature] = _mapper_\n",
    "\n",
    "        self.dview['mappers'] = self.mappers\n",
    "        return self \n",
    "    \n",
    "    def transform( self, rdf, y = None ):\n",
    "        \n",
    "        for feature in self.features:\n",
    "\n",
    "            self.dview['feature'] = feature\n",
    "\n",
    "            result = self.dview.map_sync(map_unique,rdf)\n",
    "            _uniq_ = reduce_unique(result)\n",
    "\n",
    "            #if missing assign the median value\n",
    "#             median_value = np.median(list(self.mappers[feature].values()))\n",
    "#             new_category_mapper = {cat:median_value for cat in _uniq_ if cat not in self.mappers[feature].keys()}\n",
    "\n",
    "            \n",
    "            #if missing assign 0\n",
    "            new_category_mapper = {cat:0 for cat in _uniq_ if cat not in self.mappers[feature].keys()}\n",
    "            \n",
    "            dview['new_category_mapper'] = new_category_mapper\n",
    "            if len(new_category_mapper)>0:\n",
    "                print(new_category_mapper)\n",
    "                self.dview.map_sync(map_missing_rank_transform,rdf)\n",
    "            \n",
    "            self.dview.map_sync(map_rank_transform,rdf)\n",
    "\n",
    "            \n",
    "        return pd.concat(self.dview['df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', \n",
    "       'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', \n",
    "       'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9',\n",
    "       'ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5',\n",
    "       'day', 'month','isnan']#+nom_nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a885aacec': 0}\n"
     ]
    }
   ],
   "source": [
    "rt = RankTransformParallel(dview)\n",
    "\n",
    "rdf = distribute_df(train)\n",
    "\n",
    "rt.fit(rdf,features=features,target='target')\n",
    "\n",
    "train_rank = rt.transform(rdf)[features+['target','id']]\n",
    "\n",
    "rdf = distribute_df(test)\n",
    "\n",
    "test_rank = rt.transform(rdf)[features+['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport itertools\\nfrom collections import defaultdict\\nlkup = defaultdict(lambda: defaultdict(int))\\n\\ngroups = [\\n        ['nom_5_0','nom_6_0','nom_9_0'],\\n          ['nom_5_1','nom_6_1','nom_9_1'],\\n          ['nom_5_2','nom_6_2','nom_9_2'],\\n         ]\\n\\nfor group in groups:\\n    group = list(group)\\n    print(group)\\n    \\n    tmp=train_rank[group+['target']].groupby(group).agg(['sum','mean']) \\n    tmp.columns = tmp.columns.droplevel()\\n    lkup[tuple(group)] = tmp[(tmp['mean']==1)&(tmp['sum']==1)].reset_index()[group]\\n    \\n    plt.plot(train_rank[group+['target']].groupby(group).mean().sort_values(by='target').values)\\n    plt.show()\\n    \\ntrain_leakage = []\\ntest_leakage = []\\n\\nfor group in groups:\\n\\n    trainleak = train_rank[group].apply(lambda x: np.any(np.all(x.values==lkup[tuple(group)].values,axis=1)),axis=1)\\n    testleak = test_rank[group].apply(lambda x: np.any(np.all(x.values==lkup[tuple(group)].values,axis=1)),axis=1)\\n    train_leakage.append(trainleak)\\n    test_leakage.append(testleak)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "lkup = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "groups = [\n",
    "        ['nom_5_0','nom_6_0','nom_9_0'],\n",
    "          ['nom_5_1','nom_6_1','nom_9_1'],\n",
    "          ['nom_5_2','nom_6_2','nom_9_2'],\n",
    "         ]\n",
    "\n",
    "for group in groups:\n",
    "    group = list(group)\n",
    "    print(group)\n",
    "    \n",
    "    tmp=train_rank[group+['target']].groupby(group).agg(['sum','mean']) \n",
    "    tmp.columns = tmp.columns.droplevel()\n",
    "    lkup[tuple(group)] = tmp[(tmp['mean']==1)&(tmp['sum']==1)].reset_index()[group]\n",
    "    \n",
    "    plt.plot(train_rank[group+['target']].groupby(group).mean().sort_values(by='target').values)\n",
    "    plt.show()\n",
    "    \n",
    "train_leakage = []\n",
    "test_leakage = []\n",
    "\n",
    "for group in groups:\n",
    "\n",
    "    trainleak = train_rank[group].apply(lambda x: np.any(np.all(x.values==lkup[tuple(group)].values,axis=1)),axis=1)\n",
    "    testleak = test_rank[group].apply(lambda x: np.any(np.all(x.values==lkup[tuple(group)].values,axis=1)),axis=1)\n",
    "    train_leakage.append(trainleak)\n",
    "    test_leakage.append(testleak)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 9))    \n",
    "  \n",
    "# Remove the plot frame lines. They are unnecessary chartjunk.    \n",
    "ax = plt.subplot(111)    \n",
    "ax.spines[\"top\"].set_visible(False)    \n",
    "ax.spines[\"bottom\"].set_visible(False)    \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "ax.spines[\"left\"].set_visible(False)    \n",
    "  \n",
    "# Ensure that the axis ticks only show up on the bottom and left of the plot.    \n",
    "# Ticks on the right and top of the plot are generally unnecessary chartjunk.    \n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()    \n",
    "\n",
    "plt.plot(train_rank[['nom_9','target']].groupby('nom_9').mean().sort_values(by='target').values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_features = []\n",
    "for f0 in ['nom_5','nom_6','nom_9']:\n",
    "    \n",
    "    bins = np.minimum(train_rank[f0].nunique(),256)\n",
    "    \n",
    "    XTRAIN = train_rank[f0].values.flatten().astype(np.float)\n",
    "    XTEST = test_rank[f0].values.flatten().astype(np.float)\n",
    "\n",
    "    #percentile based bins\n",
    "    percentiles_bins = np.percentile(XTRAIN,\n",
    "                                     np.linspace(0,100,bins))\n",
    "    #bin limits based on the test data\n",
    "#     _bins_ = np.linspace(np.min(XTRAIN),np.max(XTRAIN),bins)\n",
    "    \n",
    "    XTRAIN = np.digitize(XTRAIN,percentiles_bins)-1\n",
    "    XTEST = np.digitize(XTEST,percentiles_bins)-1\n",
    "    \n",
    "    train_rank[f0+'_binned'] = XTRAIN\n",
    "    test_rank[f0+'_binned'] = XTEST\n",
    "    \n",
    "    binned_features.append(f0+'_binned')\n",
    "    \n",
    "for f0 in ['nom_7','nom_8','ord_5']:\n",
    "    \n",
    "    bins = np.minimum(train_rank[f0].nunique(),128)\n",
    "    \n",
    "    XTRAIN = train_rank[f0].values.flatten().astype(np.float)\n",
    "    XTEST = test_rank[f0].values.flatten().astype(np.float)\n",
    "\n",
    "    #percentile based bins\n",
    "    percentiles_bins = np.percentile(XTRAIN,\n",
    "                                     np.linspace(0,100,bins))\n",
    "    #bin limits based on the test data\n",
    "#     _bins_ = np.linspace(np.min(XTRAIN),np.max(XTRAIN),bins)\n",
    "    \n",
    "    XTRAIN = np.digitize(XTRAIN,percentiles_bins)-1\n",
    "    XTEST = np.digitize(XTEST,percentiles_bins)-1\n",
    "    \n",
    "    train_rank[f0+'_binned'] = XTRAIN\n",
    "    test_rank[f0+'_binned'] = XTEST\n",
    "    \n",
    "    binned_features.append(f0+'_binned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 9))    \n",
    "  \n",
    "# Remove the plot frame lines. They are unnecessary chartjunk.    \n",
    "ax = plt.subplot(111)    \n",
    "ax.spines[\"top\"].set_visible(False)    \n",
    "ax.spines[\"bottom\"].set_visible(False)    \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "ax.spines[\"left\"].set_visible(False)    \n",
    "  \n",
    "# Ensure that the axis ticks only show up on the bottom and left of the plot.    \n",
    "# Ticks on the right and top of the plot are generally unnecessary chartjunk.    \n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()    \n",
    "\n",
    "plt.plot(train_rank[['nom_9_binned','target']].groupby('nom_9_binned').mean().sort_values(by='target').values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10723"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train\n",
    "del test\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# create interation variables\\n\\ngroup = ['bin_0_binned',\\n 'bin_1_binned',\\n 'bin_2_binned',\\n 'bin_3_binned',\\n 'bin_4_binned',\\n 'nom_3_binned',\\n 'ord_0_binned',\\n 'ord_1_binned',\\n 'ord_2_binned',\\n 'ord_3_binned',\\n 'ord_4_binned',\\n 'ord_5_binned',\\n 'day_binned',\\n 'month_binned']\\n\\nimport itertools\\nbinned_interaction = []\\n\\ngroups = list(itertools.combinations(group,2))\\nfor f0,f1 in groups:\\n    \\n    col = f0+'_'+f1\\n    print(col)\\n    \\n    binned_interaction.append(col)\\n    \\n    if col in train_rank.columns and col in test_rank.columns:\\n        continue\\n    \\n    train_rank[col] = train_rank[[f0, f1]].apply(lambda x: str(x[0])+'_'+str(x[1]) ,axis=1)\\n    test_rank[col] = test_rank[[f0, f1]].apply(lambda x: str(x[0])+'_'+str(x[1]) ,axis=1)\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# create interation variables\n",
    "\n",
    "group = ['bin_0_binned',\n",
    " 'bin_1_binned',\n",
    " 'bin_2_binned',\n",
    " 'bin_3_binned',\n",
    " 'bin_4_binned',\n",
    " 'nom_3_binned',\n",
    " 'ord_0_binned',\n",
    " 'ord_1_binned',\n",
    " 'ord_2_binned',\n",
    " 'ord_3_binned',\n",
    " 'ord_4_binned',\n",
    " 'ord_5_binned',\n",
    " 'day_binned',\n",
    " 'month_binned']\n",
    "\n",
    "import itertools\n",
    "binned_interaction = []\n",
    "\n",
    "groups = list(itertools.combinations(group,2))\n",
    "for f0,f1 in groups:\n",
    "    \n",
    "    col = f0+'_'+f1\n",
    "    print(col)\n",
    "    \n",
    "    binned_interaction.append(col)\n",
    "    \n",
    "    if col in train_rank.columns and col in test_rank.columns:\n",
    "        continue\n",
    "    \n",
    "    train_rank[col] = train_rank[[f0, f1]].apply(lambda x: str(x[0])+'_'+str(x[1]) ,axis=1)\n",
    "    test_rank[col] = test_rank[[f0, f1]].apply(lambda x: str(x[0])+'_'+str(x[1]) ,axis=1)\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor feature in binned_interaction:\\n    print(feature)\\n    feature = [feature]\\n    rt = RankTransformParallel(dview)\\n\\n    rdf = distribute_df(train_rank[feature+['target']])\\n\\n    rt.fit(rdf,features=feature,target='target')\\n\\n    train_rank[feature] = rt.transform(rdf)[feature]\\n\\n    rdf = distribute_df(test_rank[feature])\\n\\n    test_rank[feature] = rt.transform(rdf)[feature]\\n    \\n    feature = feature[0]\\n    bins = np.minimum(train_rank[feature].nunique(),16)\\n    \\n    XTRAIN = train_rank[feature].values.flatten().astype(np.float)\\n    XTEST = test_rank[feature].values.flatten().astype(np.float)\\n    \\n    #bin limits based on the test data\\n    _bins_ = np.linspace(np.min(XTRAIN),np.max(XTRAIN),bins)\\n    \\n    XTRAIN = np.digitize(XTRAIN,_bins_)-1\\n    XTEST = np.digitize(XTEST,_bins_)-1\\n    \\n    train_rank[feature] = XTRAIN\\n    test_rank[feature] = XTEST\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for feature in binned_interaction:\n",
    "    print(feature)\n",
    "    feature = [feature]\n",
    "    rt = RankTransformParallel(dview)\n",
    "\n",
    "    rdf = distribute_df(train_rank[feature+['target']])\n",
    "\n",
    "    rt.fit(rdf,features=feature,target='target')\n",
    "\n",
    "    train_rank[feature] = rt.transform(rdf)[feature]\n",
    "\n",
    "    rdf = distribute_df(test_rank[feature])\n",
    "\n",
    "    test_rank[feature] = rt.transform(rdf)[feature]\n",
    "    \n",
    "    feature = feature[0]\n",
    "    bins = np.minimum(train_rank[feature].nunique(),16)\n",
    "    \n",
    "    XTRAIN = train_rank[feature].values.flatten().astype(np.float)\n",
    "    XTEST = test_rank[feature].values.flatten().astype(np.float)\n",
    "    \n",
    "    #bin limits based on the test data\n",
    "    _bins_ = np.linspace(np.min(XTRAIN),np.max(XTRAIN),bins)\n",
    "    \n",
    "    XTRAIN = np.digitize(XTRAIN,_bins_)-1\n",
    "    XTEST = np.digitize(XTEST,_bins_)-1\n",
    "    \n",
    "    train_rank[feature] = XTRAIN\n",
    "    test_rank[feature] = XTEST\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rank.to_pickle('train_rank_binned_pro')\n",
    "test_rank.to_pickle('test_rank_binned_pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rank = pd.read_pickle('train_rank_binned_pro')\n",
    "test_rank=pd.read_pickle('test_rank_binned_pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_features = [x for x in train_rank.columns if 'target' not in x and 'id' not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Find places where the binned interations are not represented in the test dataset\\nimport re\\nfor feature in group:\\n    print(feature)\\n    match_re = re.compile(feature)\\n    reduce = np.array([x for x in binned_interaction if len(match_re.findall(x))>0])\\n    \\n    msk = np.any((train_rank.loc[:,reduce]==0).values,axis=1).astype(np.float)\\n    train_rank.loc[:,feature]=train_rank.loc[:,feature]*msk\\n    \\n    msk = np.any((test_rank.loc[:,reduce]==0).values,axis=1).astype(np.float)\\n    test_rank.loc[:,feature]=test_rank.loc[:,feature]*msk \\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Find places where the binned interations are not represented in the test dataset\n",
    "import re\n",
    "for feature in group:\n",
    "    print(feature)\n",
    "    match_re = re.compile(feature)\n",
    "    reduce = np.array([x for x in binned_interaction if len(match_re.findall(x))>0])\n",
    "    \n",
    "    msk = np.any((train_rank.loc[:,reduce]==0).values,axis=1).astype(np.float)\n",
    "    train_rank.loc[:,feature]=train_rank.loc[:,feature]*msk\n",
    "    \n",
    "    msk = np.any((test_rank.loc[:,reduce]==0).values,axis=1).astype(np.float)\n",
    "    test_rank.loc[:,feature]=test_rank.loc[:,feature]*msk \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Count of feature interations in the train and test group\\nno_info_train_msk = np.zeros(train_rank.shape[0],dtype=np.bool)\\nno_info_test_msk = np.zeros(test_rank.shape[0],dtype=np.bool)\\n\\nno_info = []\\nno_info_count = 0\\nN=0\\ngroups = list(itertools.combinations(['nom_5','nom_6_0'],2))\\nprint(len(groups))\\nfor group in groups:\\n\\n    group = list(group)\\n#     group = ['nom_8', 'nom_9']\\n    percent = train_rank[group+['target']].groupby(group).mean()\\n    count = train_rank[group+['target']].groupby(group).sum()\\n\\n\\n    tmp = percent[(percent['target']==1)&(count['target']==1)]\\n\\n    if tmp.shape[0]>0:\\n        no_info_count+=tmp.shape[0]\\n        arr = tmp.reset_index()[group].values\\n\\n        test_msk = test_rank[group].apply(tuple, axis = 1).isin(tuple(map(tuple, arr)))\\n        train_msk = train_rank[group].apply(tuple, axis = 1).isin(tuple(map(tuple, arr)))\\n        no_info_test_msk=no_info_test_msk|test_msk\\n        no_info_train_msk=no_info_train_msk|train_msk\\n        print('_'.join(group))\\n        print(np.sum(train_msk),np.sum(test_msk),N)\\n\\n        no_info.append([group,tuple(map(tuple, arr)),train_msk,test_msk])\\n    N+=1\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Count of feature interations in the train and test group\n",
    "no_info_train_msk = np.zeros(train_rank.shape[0],dtype=np.bool)\n",
    "no_info_test_msk = np.zeros(test_rank.shape[0],dtype=np.bool)\n",
    "\n",
    "no_info = []\n",
    "no_info_count = 0\n",
    "N=0\n",
    "groups = list(itertools.combinations(['nom_5','nom_6_0'],2))\n",
    "print(len(groups))\n",
    "for group in groups:\n",
    "\n",
    "    group = list(group)\n",
    "#     group = ['nom_8', 'nom_9']\n",
    "    percent = train_rank[group+['target']].groupby(group).mean()\n",
    "    count = train_rank[group+['target']].groupby(group).sum()\n",
    "\n",
    "\n",
    "    tmp = percent[(percent['target']==1)&(count['target']==1)]\n",
    "\n",
    "    if tmp.shape[0]>0:\n",
    "        no_info_count+=tmp.shape[0]\n",
    "        arr = tmp.reset_index()[group].values\n",
    "\n",
    "        test_msk = test_rank[group].apply(tuple, axis = 1).isin(tuple(map(tuple, arr)))\n",
    "        train_msk = train_rank[group].apply(tuple, axis = 1).isin(tuple(map(tuple, arr)))\n",
    "        no_info_test_msk=no_info_test_msk|test_msk\n",
    "        no_info_train_msk=no_info_train_msk|train_msk\n",
    "        print('_'.join(group))\n",
    "        print(np.sum(train_msk),np.sum(test_msk),N)\n",
    "\n",
    "        no_info.append([group,tuple(map(tuple, arr)),train_msk,test_msk])\n",
    "    N+=1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample_df(df, col, n_samples):\n",
    "    n = min(n_samples, df[col].value_counts().min())\n",
    "    df_ = df.groupby(col).apply(lambda x: x.sample(n))\n",
    "    df_.index = df_.index.droplevel(0)\n",
    "    return df_,df_.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rank['target_0']=train_rank['target']==0\n",
    "train_rank['target_1']=train_rank['target']==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rank.reset_index(drop=True,inplace=True)\n",
    "test_rank.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatEmbeddingBlock(tf.keras.Model):\n",
    "    def __init__(self,n_uniq,embed_dim):\n",
    "        super(CatEmbeddingBlock,self).__init__(name='')\n",
    "        self.emb = tf.keras.layers.Embedding(n_uniq + 1, embed_dim)\n",
    "        self.drop = tf.keras.layers.SpatialDropout1D(0.1)\n",
    "        self.out = tf.keras.layers.Reshape(target_shape=(embed_dim, ))\n",
    "            \n",
    "    def call(self,input_data,training=False):\n",
    "\n",
    "        emb = self.emb(input_data)\n",
    "        drop = self.drop(emb)\n",
    "        out = self.out(drop)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_features = [x for x in train_rank.columns if 'target' not in x and 'id' not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_features = np.array(emb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mkl.set_num_threads(6)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "inputs = []\n",
    "embed_layers = []\n",
    "embed_dims = []\n",
    "num_unique_values = []\n",
    "for i in range(train_rank[emb_features].shape[1]):\n",
    "    print(i)\n",
    "    inp = tf.keras.layers.Input(shape=(1,))\n",
    "    \n",
    "    n_uniq = np.max(train_rank[emb_features].values[:,i]).astype(np.int)\n",
    "#     embed_dim = int(min(np.ceil((n_uniq)/2), 32))\n",
    "    embed_dim = int(min(n_uniq, 32))\n",
    "    \n",
    "    layer = CatEmbeddingBlock(n_uniq,embed_dim)(inp)\n",
    "    \n",
    "    \n",
    "    inputs.append(inp)\n",
    "    embed_layers.append(layer)\n",
    "    \n",
    "    embed_dims.append(embed_dim)\n",
    "    num_unique_values.append(n_uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'nom_0', 'nom_1', 'nom_2',\n",
       "       'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_0',\n",
       "       'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5', 'day', 'month', 'isnan',\n",
       "       'id', 'nom_5_binned', 'nom_6_binned', 'nom_9_binned', 'nom_7_binned',\n",
       "       'nom_8_binned', 'ord_5_binned'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rank.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = len(emb_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_input = tf.keras.Input(shape=(D))\n",
    "# e = tf.keras.layers.Concatenate()(embed_layers+[raw_input])\n",
    "\n",
    "e = tf.keras.layers.Concatenate()(embed_layers)\n",
    "e = tf.keras.layers.BatchNormalization()(e)\n",
    "e = tf.keras.layers.Dense(300, activation=\"relu\")(e)\n",
    "e = tf.keras.layers.Dropout(0.3)(e)\n",
    "e = tf.keras.layers.BatchNormalization()(e)\n",
    "e = tf.keras.layers.Dense(300, activation=\"relu\")(e)\n",
    "e = tf.keras.layers.Dropout(0.3)(e)\n",
    "e = tf.keras.layers.BatchNormalization()(e)\n",
    "\n",
    "\n",
    "output = tf.keras.layers.Dense(2, activation=\"softmax\")(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Model(inputs+[raw_input], output)\n",
    "model = tf.keras.Model(inputs, output)\n",
    "\n",
    "# concat_layers.shape\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy',tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_W = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_rank['target_0']=train_rank['target']==0\n",
    "train_rank['target_1']=train_rank['target']==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample_df(df, col, n_samples):\n",
    "    n = min(n_samples, df[col].value_counts().min())\n",
    "    df_ = df.groupby(col).apply(lambda x: x.sample(n))\n",
    "    df_.index = df_.index.droplevel(0)\n",
    "    return df_.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohankotwani/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5989 - accuracy: 0.6858 - auc: 0.7496 - val_loss: 0.5573 - val_accuracy: 0.7080 - val_auc: 0.7843\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 12s 59us/sample - loss: 0.5559 - accuracy: 0.7134 - auc: 0.7869 - val_loss: 0.5569 - val_accuracy: 0.7123 - val_auc: 0.7858\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 12s 59us/sample - loss: 0.5479 - accuracy: 0.7187 - auc: 0.7942 - val_loss: 0.5590 - val_accuracy: 0.7117 - val_auc: 0.7836\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 12s 59us/sample - loss: 0.5394 - accuracy: 0.7255 - auc: 0.8017 - val_loss: 0.5608 - val_accuracy: 0.7068 - val_auc: 0.7819\n",
      "Epoch 5/100\n",
      "195584/196565 [============================>.] - ETA: 0s - loss: 0.5277 - accuracy: 0.7333 - auc: 0.8120\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5277 - accuracy: 0.7333 - auc: 0.8120 - val_loss: 0.5681 - val_accuracy: 0.7062 - val_auc: 0.7768\n",
      "Epoch 6/100\n",
      "196565/196565 [==============================] - 12s 59us/sample - loss: 0.5087 - accuracy: 0.7469 - auc: 0.8276 - val_loss: 0.5818 - val_accuracy: 0.7033 - val_auc: 0.7715\n",
      "Epoch 7/100\n",
      "196352/196565 [============================>.] - ETA: 0s - loss: 0.4854 - accuracy: 0.7631 - auc: 0.8455Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.4854 - accuracy: 0.7631 - auc: 0.8454 - val_loss: 0.5903 - val_accuracy: 0.6931 - val_auc: 0.7624\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohankotwani/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 62us/sample - loss: 0.5957 - accuracy: 0.6867 - auc: 0.7520 - val_loss: 0.5641 - val_accuracy: 0.7039 - val_auc: 0.7778\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 12s 61us/sample - loss: 0.5559 - accuracy: 0.7131 - auc: 0.7867 - val_loss: 0.5623 - val_accuracy: 0.7059 - val_auc: 0.7797\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 13s 65us/sample - loss: 0.5470 - accuracy: 0.7200 - auc: 0.7951 - val_loss: 0.5638 - val_accuracy: 0.7057 - val_auc: 0.7784\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 12s 62us/sample - loss: 0.5385 - accuracy: 0.7256 - auc: 0.8024 - val_loss: 0.5674 - val_accuracy: 0.7023 - val_auc: 0.7751\n",
      "Epoch 5/100\n",
      "195584/196565 [============================>.] - ETA: 0s - loss: 0.5272 - accuracy: 0.7339 - auc: 0.8124\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 13s 66us/sample - loss: 0.5272 - accuracy: 0.7340 - auc: 0.8125 - val_loss: 0.5749 - val_accuracy: 0.7000 - val_auc: 0.7705\n",
      "Epoch 6/100\n",
      "196565/196565 [==============================] - 13s 64us/sample - loss: 0.5097 - accuracy: 0.7465 - auc: 0.8270 - val_loss: 0.5833 - val_accuracy: 0.6957 - val_auc: 0.7655\n",
      "Epoch 7/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.4842 - accuracy: 0.7630 - auc: 0.8461Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 12s 59us/sample - loss: 0.4842 - accuracy: 0.7630 - auc: 0.8462 - val_loss: 0.6036 - val_accuracy: 0.6885 - val_auc: 0.7592\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 62us/sample - loss: 0.5961 - accuracy: 0.6858 - auc: 0.7511 - val_loss: 0.5575 - val_accuracy: 0.7130 - val_auc: 0.7847\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 12s 59us/sample - loss: 0.5556 - accuracy: 0.7134 - auc: 0.7871 - val_loss: 0.5582 - val_accuracy: 0.7120 - val_auc: 0.7837\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 11s 58us/sample - loss: 0.5478 - accuracy: 0.7195 - auc: 0.7945 - val_loss: 0.5614 - val_accuracy: 0.7090 - val_auc: 0.7826\n",
      "Epoch 4/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.5400 - accuracy: 0.7245 - auc: 0.8014\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 11s 58us/sample - loss: 0.5401 - accuracy: 0.7244 - auc: 0.8013 - val_loss: 0.5618 - val_accuracy: 0.7097 - val_auc: 0.7812\n",
      "Epoch 5/100\n",
      "196565/196565 [==============================] - 11s 58us/sample - loss: 0.5289 - accuracy: 0.7318 - auc: 0.8110 - val_loss: 0.5690 - val_accuracy: 0.7027 - val_auc: 0.7739\n",
      "Epoch 6/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.5105 - accuracy: 0.7464 - auc: 0.8264Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 11s 57us/sample - loss: 0.5104 - accuracy: 0.7464 - auc: 0.8265 - val_loss: 0.5796 - val_accuracy: 0.6979 - val_auc: 0.7675\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5962 - accuracy: 0.6872 - auc: 0.7518 - val_loss: 0.5574 - val_accuracy: 0.7099 - val_auc: 0.7846\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 12s 62us/sample - loss: 0.5555 - accuracy: 0.7145 - auc: 0.7871 - val_loss: 0.5561 - val_accuracy: 0.7120 - val_auc: 0.7870\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 12s 62us/sample - loss: 0.5471 - accuracy: 0.7192 - auc: 0.7948 - val_loss: 0.5595 - val_accuracy: 0.7112 - val_auc: 0.7840\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 12s 62us/sample - loss: 0.5396 - accuracy: 0.7241 - auc: 0.8015 - val_loss: 0.5618 - val_accuracy: 0.7109 - val_auc: 0.7825\n",
      "Epoch 5/100\n",
      "196096/196565 [============================>.] - ETA: 0s - loss: 0.5261 - accuracy: 0.7345 - auc: 0.8132\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 12s 62us/sample - loss: 0.5261 - accuracy: 0.7344 - auc: 0.8132 - val_loss: 0.5705 - val_accuracy: 0.7042 - val_auc: 0.7779\n",
      "Epoch 6/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5076 - accuracy: 0.7480 - auc: 0.8286 - val_loss: 0.5818 - val_accuracy: 0.6990 - val_auc: 0.7690\n",
      "Epoch 7/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.4830 - accuracy: 0.7645 - auc: 0.8471Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.4831 - accuracy: 0.7645 - auc: 0.8471 - val_loss: 0.6046 - val_accuracy: 0.6921 - val_auc: 0.7621\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 61us/sample - loss: 0.5963 - accuracy: 0.6872 - auc: 0.7516 - val_loss: 0.5597 - val_accuracy: 0.7060 - val_auc: 0.7817\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 12s 63us/sample - loss: 0.5572 - accuracy: 0.7128 - auc: 0.7858 - val_loss: 0.5584 - val_accuracy: 0.7072 - val_auc: 0.7837\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 12s 61us/sample - loss: 0.5481 - accuracy: 0.7190 - auc: 0.7942 - val_loss: 0.5587 - val_accuracy: 0.7085 - val_auc: 0.7832\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 12s 61us/sample - loss: 0.5403 - accuracy: 0.7240 - auc: 0.8010 - val_loss: 0.5644 - val_accuracy: 0.7069 - val_auc: 0.7796\n",
      "Epoch 5/100\n",
      "196352/196565 [============================>.] - ETA: 0s - loss: 0.5286 - accuracy: 0.7338 - auc: 0.8116\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 12s 62us/sample - loss: 0.5286 - accuracy: 0.7337 - auc: 0.8115 - val_loss: 0.5691 - val_accuracy: 0.7022 - val_auc: 0.7755\n",
      "Epoch 6/100\n",
      "196565/196565 [==============================] - 12s 62us/sample - loss: 0.5109 - accuracy: 0.7459 - auc: 0.8259 - val_loss: 0.5770 - val_accuracy: 0.6991 - val_auc: 0.7680\n",
      "Epoch 7/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.4873 - accuracy: 0.7633 - auc: 0.8445Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 13s 66us/sample - loss: 0.4873 - accuracy: 0.7633 - auc: 0.8444 - val_loss: 0.5973 - val_accuracy: 0.6951 - val_auc: 0.7625\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 61us/sample - loss: 0.5960 - accuracy: 0.6872 - auc: 0.7517 - val_loss: 0.5568 - val_accuracy: 0.7110 - val_auc: 0.7862\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 12s 61us/sample - loss: 0.5570 - accuracy: 0.7124 - auc: 0.7858 - val_loss: 0.5566 - val_accuracy: 0.7118 - val_auc: 0.7867\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 12s 62us/sample - loss: 0.5475 - accuracy: 0.7195 - auc: 0.7947 - val_loss: 0.5577 - val_accuracy: 0.7114 - val_auc: 0.7852\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 13s 64us/sample - loss: 0.5397 - accuracy: 0.7252 - auc: 0.8017 - val_loss: 0.5623 - val_accuracy: 0.7060 - val_auc: 0.7807\n",
      "Epoch 5/100\n",
      "196096/196565 [============================>.] - ETA: 0s - loss: 0.5289 - accuracy: 0.7325 - auc: 0.8109\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 13s 65us/sample - loss: 0.5289 - accuracy: 0.7325 - auc: 0.8108 - val_loss: 0.5731 - val_accuracy: 0.7082 - val_auc: 0.7779\n",
      "Epoch 6/100\n",
      "196565/196565 [==============================] - 13s 64us/sample - loss: 0.5111 - accuracy: 0.7460 - auc: 0.8259 - val_loss: 0.5779 - val_accuracy: 0.6995 - val_auc: 0.7689\n",
      "Epoch 7/100\n",
      "195584/196565 [============================>.] - ETA: 0s - loss: 0.4865 - accuracy: 0.7622 - auc: 0.8448Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 12s 61us/sample - loss: 0.4866 - accuracy: 0.7621 - auc: 0.8447 - val_loss: 0.5923 - val_accuracy: 0.6920 - val_auc: 0.7591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196566 samples, validate on 28080 samples\n",
      "Epoch 1/100\n",
      "196566/196566 [==============================] - 12s 62us/sample - loss: 0.5962 - accuracy: 0.6873 - auc: 0.7517 - val_loss: 0.5554 - val_accuracy: 0.7101 - val_auc: 0.7870\n",
      "Epoch 2/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5562 - accuracy: 0.7129 - auc: 0.7865 - val_loss: 0.5550 - val_accuracy: 0.7112 - val_auc: 0.7872\n",
      "Epoch 3/100\n",
      "196566/196566 [==============================] - 12s 61us/sample - loss: 0.5478 - accuracy: 0.7186 - auc: 0.7941 - val_loss: 0.5585 - val_accuracy: 0.7104 - val_auc: 0.7852\n",
      "Epoch 4/100\n",
      "196566/196566 [==============================] - 13s 66us/sample - loss: 0.5399 - accuracy: 0.7234 - auc: 0.8012 - val_loss: 0.5604 - val_accuracy: 0.7098 - val_auc: 0.7841\n",
      "Epoch 5/100\n",
      "196352/196566 [============================>.] - ETA: 0s - loss: 0.5289 - accuracy: 0.7324 - auc: 0.8110\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196566/196566 [==============================] - 12s 63us/sample - loss: 0.5288 - accuracy: 0.7325 - auc: 0.8110 - val_loss: 0.5680 - val_accuracy: 0.7068 - val_auc: 0.7798\n",
      "Epoch 6/100\n",
      "196566/196566 [==============================] - 12s 63us/sample - loss: 0.5104 - accuracy: 0.7462 - auc: 0.8263 - val_loss: 0.5741 - val_accuracy: 0.7024 - val_auc: 0.7733\n",
      "Epoch 7/100\n",
      "195584/196566 [============================>.] - ETA: 0s - loss: 0.4870 - accuracy: 0.7620 - auc: 0.8445Restoring model weights from the end of the best epoch.\n",
      "196566/196566 [==============================] - 13s 64us/sample - loss: 0.4871 - accuracy: 0.7620 - auc: 0.8444 - val_loss: 0.5930 - val_accuracy: 0.6950 - val_auc: 0.7646\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196566 samples, validate on 28080 samples\n",
      "Epoch 1/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5953 - accuracy: 0.6882 - auc: 0.7528 - val_loss: 0.5647 - val_accuracy: 0.7040 - val_auc: 0.7778\n",
      "Epoch 2/100\n",
      "196566/196566 [==============================] - 12s 61us/sample - loss: 0.5556 - accuracy: 0.7122 - auc: 0.7869 - val_loss: 0.5627 - val_accuracy: 0.7048 - val_auc: 0.7789\n",
      "Epoch 3/100\n",
      "196566/196566 [==============================] - 13s 64us/sample - loss: 0.5468 - accuracy: 0.7204 - auc: 0.7954 - val_loss: 0.5653 - val_accuracy: 0.7054 - val_auc: 0.7782\n",
      "Epoch 4/100\n",
      "196566/196566 [==============================] - 13s 64us/sample - loss: 0.5387 - accuracy: 0.7266 - auc: 0.8027 - val_loss: 0.5721 - val_accuracy: 0.7010 - val_auc: 0.7742\n",
      "Epoch 5/100\n",
      "196352/196566 [============================>.] - ETA: 0s - loss: 0.5262 - accuracy: 0.7340 - auc: 0.8133\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196566/196566 [==============================] - 13s 65us/sample - loss: 0.5263 - accuracy: 0.7340 - auc: 0.8133 - val_loss: 0.5832 - val_accuracy: 0.6957 - val_auc: 0.7694\n",
      "Epoch 6/100\n",
      "196566/196566 [==============================] - 12s 61us/sample - loss: 0.5082 - accuracy: 0.7480 - auc: 0.8283 - val_loss: 0.5841 - val_accuracy: 0.6932 - val_auc: 0.7625\n",
      "Epoch 7/100\n",
      "196096/196566 [============================>.] - ETA: 0s - loss: 0.4840 - accuracy: 0.7636 - auc: 0.8466Restoring model weights from the end of the best epoch.\n",
      "196566/196566 [==============================] - 13s 68us/sample - loss: 0.4840 - accuracy: 0.7636 - auc: 0.8466 - val_loss: 0.6103 - val_accuracy: 0.6836 - val_auc: 0.7523\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 13s 67us/sample - loss: 0.5972 - accuracy: 0.6878 - auc: 0.7513 - val_loss: 0.5597 - val_accuracy: 0.7095 - val_auc: 0.7843\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 13s 64us/sample - loss: 0.5570 - accuracy: 0.7126 - auc: 0.7860 - val_loss: 0.5569 - val_accuracy: 0.7101 - val_auc: 0.7848\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5480 - accuracy: 0.7194 - auc: 0.7943 - val_loss: 0.5603 - val_accuracy: 0.7094 - val_auc: 0.7836\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 13s 65us/sample - loss: 0.5400 - accuracy: 0.7257 - auc: 0.8015 - val_loss: 0.5623 - val_accuracy: 0.7098 - val_auc: 0.7817\n",
      "Epoch 5/100\n",
      "196096/196565 [============================>.] - ETA: 0s - loss: 0.5290 - accuracy: 0.7337 - auc: 0.8112\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 13s 64us/sample - loss: 0.5290 - accuracy: 0.7336 - auc: 0.8112 - val_loss: 0.5706 - val_accuracy: 0.7039 - val_auc: 0.7763\n",
      "Epoch 6/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5113 - accuracy: 0.7449 - auc: 0.8257 - val_loss: 0.5955 - val_accuracy: 0.7010 - val_auc: 0.7701\n",
      "Epoch 7/100\n",
      "196096/196565 [============================>.] - ETA: 0s - loss: 0.4875 - accuracy: 0.7624 - auc: 0.8441Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 12s 61us/sample - loss: 0.4875 - accuracy: 0.7624 - auc: 0.8441 - val_loss: 0.5926 - val_accuracy: 0.6922 - val_auc: 0.7621\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 61us/sample - loss: 0.5973 - accuracy: 0.6857 - auc: 0.7507 - val_loss: 0.5577 - val_accuracy: 0.7101 - val_auc: 0.7849\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 13s 65us/sample - loss: 0.5578 - accuracy: 0.7122 - auc: 0.7853 - val_loss: 0.5541 - val_accuracy: 0.7105 - val_auc: 0.7872\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 13s 68us/sample - loss: 0.5481 - accuracy: 0.7198 - auc: 0.7944 - val_loss: 0.5564 - val_accuracy: 0.7090 - val_auc: 0.7848\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 13s 65us/sample - loss: 0.5398 - accuracy: 0.7245 - auc: 0.8014 - val_loss: 0.5612 - val_accuracy: 0.7054 - val_auc: 0.7808\n",
      "Epoch 5/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.5286 - accuracy: 0.7330 - auc: 0.8113\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 13s 67us/sample - loss: 0.5286 - accuracy: 0.7329 - auc: 0.8113 - val_loss: 0.5671 - val_accuracy: 0.7029 - val_auc: 0.7770\n",
      "Epoch 6/100\n",
      "196565/196565 [==============================] - 14s 69us/sample - loss: 0.5110 - accuracy: 0.7458 - auc: 0.8260 - val_loss: 0.5833 - val_accuracy: 0.6986 - val_auc: 0.7695\n",
      "Epoch 7/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.4863 - accuracy: 0.7631 - auc: 0.8451Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 14s 70us/sample - loss: 0.4864 - accuracy: 0.7630 - auc: 0.8450 - val_loss: 0.5972 - val_accuracy: 0.6925 - val_auc: 0.7619\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 13s 68us/sample - loss: 0.5967 - accuracy: 0.6861 - auc: 0.7513 - val_loss: 0.5626 - val_accuracy: 0.7069 - val_auc: 0.7794\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 13s 67us/sample - loss: 0.5555 - accuracy: 0.7140 - auc: 0.7873 - val_loss: 0.5682 - val_accuracy: 0.7091 - val_auc: 0.7805\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 12s 63us/sample - loss: 0.5468 - accuracy: 0.7195 - auc: 0.7951 - val_loss: 0.5651 - val_accuracy: 0.7078 - val_auc: 0.7778\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 12s 63us/sample - loss: 0.5384 - accuracy: 0.7261 - auc: 0.8028 - val_loss: 0.5664 - val_accuracy: 0.7072 - val_auc: 0.7779\n",
      "Epoch 5/100\n",
      "196096/196565 [============================>.] - ETA: 0s - loss: 0.5261 - accuracy: 0.7352 - auc: 0.8135\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 12s 62us/sample - loss: 0.5262 - accuracy: 0.7352 - auc: 0.8135 - val_loss: 0.5790 - val_accuracy: 0.6995 - val_auc: 0.7691\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196565/196565 [==============================] - 12s 64us/sample - loss: 0.5070 - accuracy: 0.7485 - auc: 0.8291 - val_loss: 0.5894 - val_accuracy: 0.6962 - val_auc: 0.7649\n",
      "Epoch 7/100\n",
      "196096/196565 [============================>.] - ETA: 0s - loss: 0.4814 - accuracy: 0.7658 - auc: 0.8483Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 12s 61us/sample - loss: 0.4815 - accuracy: 0.7657 - auc: 0.8482 - val_loss: 0.6106 - val_accuracy: 0.6866 - val_auc: 0.7535\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5963 - accuracy: 0.6862 - auc: 0.7513 - val_loss: 0.5625 - val_accuracy: 0.7093 - val_auc: 0.7840\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 12s 59us/sample - loss: 0.5563 - accuracy: 0.7126 - auc: 0.7863 - val_loss: 0.5563 - val_accuracy: 0.7106 - val_auc: 0.7862\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 12s 61us/sample - loss: 0.5481 - accuracy: 0.7191 - auc: 0.7940 - val_loss: 0.5593 - val_accuracy: 0.7100 - val_auc: 0.7852\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5399 - accuracy: 0.7251 - auc: 0.8014 - val_loss: 0.5624 - val_accuracy: 0.7065 - val_auc: 0.7804\n",
      "Epoch 5/100\n",
      "195584/196565 [============================>.] - ETA: 0s - loss: 0.5289 - accuracy: 0.7319 - auc: 0.8110\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5290 - accuracy: 0.7319 - auc: 0.8110 - val_loss: 0.5709 - val_accuracy: 0.7063 - val_auc: 0.7758\n",
      "Epoch 6/100\n",
      "196565/196565 [==============================] - 12s 62us/sample - loss: 0.5111 - accuracy: 0.7451 - auc: 0.8256 - val_loss: 0.5823 - val_accuracy: 0.6994 - val_auc: 0.7697\n",
      "Epoch 7/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.4862 - accuracy: 0.7630 - auc: 0.8450Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.4863 - accuracy: 0.7630 - auc: 0.8450 - val_loss: 0.5941 - val_accuracy: 0.6940 - val_auc: 0.7636\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5967 - accuracy: 0.6855 - auc: 0.7506 - val_loss: 0.5666 - val_accuracy: 0.7070 - val_auc: 0.7814\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5565 - accuracy: 0.7123 - auc: 0.7863 - val_loss: 0.5593 - val_accuracy: 0.7103 - val_auc: 0.7829\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5482 - accuracy: 0.7189 - auc: 0.7942 - val_loss: 0.5614 - val_accuracy: 0.7094 - val_auc: 0.7816\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5402 - accuracy: 0.7244 - auc: 0.8012 - val_loss: 0.5688 - val_accuracy: 0.7063 - val_auc: 0.7798\n",
      "Epoch 5/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.5290 - accuracy: 0.7328 - auc: 0.8108\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5291 - accuracy: 0.7327 - auc: 0.8107 - val_loss: 0.5696 - val_accuracy: 0.7059 - val_auc: 0.7761\n",
      "Epoch 6/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5116 - accuracy: 0.7461 - auc: 0.8256 - val_loss: 0.5804 - val_accuracy: 0.7007 - val_auc: 0.7686\n",
      "Epoch 7/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.4876 - accuracy: 0.7612 - auc: 0.8438Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 12s 61us/sample - loss: 0.4876 - accuracy: 0.7612 - auc: 0.8439 - val_loss: 0.5971 - val_accuracy: 0.6958 - val_auc: 0.7615\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5967 - accuracy: 0.6874 - auc: 0.7515 - val_loss: 0.5587 - val_accuracy: 0.7076 - val_auc: 0.7827\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5564 - accuracy: 0.7138 - auc: 0.7865 - val_loss: 0.5568 - val_accuracy: 0.7107 - val_auc: 0.7849\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5484 - accuracy: 0.7188 - auc: 0.7939 - val_loss: 0.5591 - val_accuracy: 0.7091 - val_auc: 0.7829\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5408 - accuracy: 0.7250 - auc: 0.8009 - val_loss: 0.5626 - val_accuracy: 0.7068 - val_auc: 0.7799\n",
      "Epoch 5/100\n",
      "195584/196565 [============================>.] - ETA: 0s - loss: 0.5287 - accuracy: 0.7325 - auc: 0.8111\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5288 - accuracy: 0.7324 - auc: 0.8111 - val_loss: 0.5707 - val_accuracy: 0.7033 - val_auc: 0.7744\n",
      "Epoch 6/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5105 - accuracy: 0.7468 - auc: 0.8264 - val_loss: 0.5898 - val_accuracy: 0.6998 - val_auc: 0.7682\n",
      "Epoch 7/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.4862 - accuracy: 0.7630 - auc: 0.8450Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.4862 - accuracy: 0.7631 - auc: 0.8451 - val_loss: 0.6028 - val_accuracy: 0.6924 - val_auc: 0.7590\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196566 samples, validate on 28080 samples\n",
      "Epoch 1/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5964 - accuracy: 0.6872 - auc: 0.7517 - val_loss: 0.5627 - val_accuracy: 0.7087 - val_auc: 0.7804\n",
      "Epoch 2/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5560 - accuracy: 0.7137 - auc: 0.7869 - val_loss: 0.5649 - val_accuracy: 0.7077 - val_auc: 0.7792\n",
      "Epoch 3/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5473 - accuracy: 0.7191 - auc: 0.7948 - val_loss: 0.5645 - val_accuracy: 0.7087 - val_auc: 0.7787\n",
      "Epoch 4/100\n",
      "195840/196566 [============================>.] - ETA: 0s - loss: 0.5388 - accuracy: 0.7255 - auc: 0.8025\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5389 - accuracy: 0.7255 - auc: 0.8024 - val_loss: 0.5685 - val_accuracy: 0.7033 - val_auc: 0.7758\n",
      "Epoch 5/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5276 - accuracy: 0.7342 - auc: 0.8123 - val_loss: 0.5749 - val_accuracy: 0.7027 - val_auc: 0.7723\n",
      "Epoch 6/100\n",
      "195840/196566 [============================>.] - ETA: 0s - loss: 0.5098 - accuracy: 0.7466 - auc: 0.8269Restoring model weights from the end of the best epoch.\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5097 - accuracy: 0.7467 - auc: 0.8269 - val_loss: 0.5862 - val_accuracy: 0.6964 - val_auc: 0.7643\n",
      "Epoch 00006: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196566 samples, validate on 28080 samples\n",
      "Epoch 1/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5976 - accuracy: 0.6860 - auc: 0.7505 - val_loss: 0.5589 - val_accuracy: 0.7090 - val_auc: 0.7840\n",
      "Epoch 2/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5570 - accuracy: 0.7121 - auc: 0.7858 - val_loss: 0.5583 - val_accuracy: 0.7100 - val_auc: 0.7844\n",
      "Epoch 3/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5480 - accuracy: 0.7190 - auc: 0.7941 - val_loss: 0.5597 - val_accuracy: 0.7085 - val_auc: 0.7830\n",
      "Epoch 4/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5404 - accuracy: 0.7237 - auc: 0.8009 - val_loss: 0.5651 - val_accuracy: 0.7073 - val_auc: 0.7808\n",
      "Epoch 5/100\n",
      "196352/196566 [============================>.] - ETA: 0s - loss: 0.5286 - accuracy: 0.7323 - auc: 0.8112\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5286 - accuracy: 0.7324 - auc: 0.8112 - val_loss: 0.5691 - val_accuracy: 0.7032 - val_auc: 0.7762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "196566/196566 [==============================] - 12s 59us/sample - loss: 0.5100 - accuracy: 0.7467 - auc: 0.8267 - val_loss: 0.5841 - val_accuracy: 0.6959 - val_auc: 0.7686\n",
      "Epoch 7/100\n",
      "195840/196566 [============================>.] - ETA: 0s - loss: 0.4854 - accuracy: 0.7632 - auc: 0.8457Restoring model weights from the end of the best epoch.\n",
      "196566/196566 [==============================] - 12s 59us/sample - loss: 0.4855 - accuracy: 0.7631 - auc: 0.8456 - val_loss: 0.6003 - val_accuracy: 0.6898 - val_auc: 0.7582\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5986 - accuracy: 0.6847 - auc: 0.7489 - val_loss: 0.5610 - val_accuracy: 0.7079 - val_auc: 0.7815\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5578 - accuracy: 0.7113 - auc: 0.7848 - val_loss: 0.5601 - val_accuracy: 0.7098 - val_auc: 0.7830\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5487 - accuracy: 0.7182 - auc: 0.7931 - val_loss: 0.5637 - val_accuracy: 0.7087 - val_auc: 0.7797\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5406 - accuracy: 0.7227 - auc: 0.8001 - val_loss: 0.5722 - val_accuracy: 0.7038 - val_auc: 0.7755\n",
      "Epoch 5/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.5301 - accuracy: 0.7315 - auc: 0.8096\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5301 - accuracy: 0.7315 - auc: 0.8096 - val_loss: 0.5773 - val_accuracy: 0.7022 - val_auc: 0.7697\n",
      "Epoch 6/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5134 - accuracy: 0.7438 - auc: 0.8237 - val_loss: 0.5913 - val_accuracy: 0.6967 - val_auc: 0.7638\n",
      "Epoch 7/100\n",
      "196352/196565 [============================>.] - ETA: 0s - loss: 0.4890 - accuracy: 0.7600 - auc: 0.8426Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.4890 - accuracy: 0.7600 - auc: 0.8426 - val_loss: 0.5992 - val_accuracy: 0.6905 - val_auc: 0.7562\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5980 - accuracy: 0.6851 - auc: 0.7491 - val_loss: 0.5589 - val_accuracy: 0.7067 - val_auc: 0.7829\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5586 - accuracy: 0.7105 - auc: 0.7840 - val_loss: 0.5580 - val_accuracy: 0.7075 - val_auc: 0.7835\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5501 - accuracy: 0.7180 - auc: 0.7923 - val_loss: 0.5601 - val_accuracy: 0.7078 - val_auc: 0.7812\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5422 - accuracy: 0.7221 - auc: 0.7990 - val_loss: 0.5677 - val_accuracy: 0.7077 - val_auc: 0.7801\n",
      "Epoch 5/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.5304 - accuracy: 0.7312 - auc: 0.8096\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5305 - accuracy: 0.7312 - auc: 0.8095 - val_loss: 0.5709 - val_accuracy: 0.7046 - val_auc: 0.7736\n",
      "Epoch 6/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5132 - accuracy: 0.7446 - auc: 0.8239 - val_loss: 0.5775 - val_accuracy: 0.6973 - val_auc: 0.7681\n",
      "Epoch 7/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.4886 - accuracy: 0.7610 - auc: 0.8431Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.4886 - accuracy: 0.7610 - auc: 0.8430 - val_loss: 0.6022 - val_accuracy: 0.6923 - val_auc: 0.7603\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5978 - accuracy: 0.6854 - auc: 0.7497 - val_loss: 0.5616 - val_accuracy: 0.7055 - val_auc: 0.7799\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5577 - accuracy: 0.7124 - auc: 0.7850 - val_loss: 0.5623 - val_accuracy: 0.7068 - val_auc: 0.7824\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5486 - accuracy: 0.7179 - auc: 0.7933 - val_loss: 0.5634 - val_accuracy: 0.7057 - val_auc: 0.7795\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5407 - accuracy: 0.7239 - auc: 0.8004 - val_loss: 0.5657 - val_accuracy: 0.7046 - val_auc: 0.7767\n",
      "Epoch 5/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.5287 - accuracy: 0.7321 - auc: 0.8110\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 12s 59us/sample - loss: 0.5288 - accuracy: 0.7320 - auc: 0.8109 - val_loss: 0.5736 - val_accuracy: 0.7015 - val_auc: 0.7712\n",
      "Epoch 6/100\n",
      "196565/196565 [==============================] - 12s 59us/sample - loss: 0.5099 - accuracy: 0.7469 - auc: 0.8267 - val_loss: 0.5860 - val_accuracy: 0.6928 - val_auc: 0.7633\n",
      "Epoch 7/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.4859 - accuracy: 0.7625 - auc: 0.8452Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 12s 59us/sample - loss: 0.4861 - accuracy: 0.7624 - auc: 0.8451 - val_loss: 0.5995 - val_accuracy: 0.6872 - val_auc: 0.7539\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5982 - accuracy: 0.6838 - auc: 0.7488 - val_loss: 0.5596 - val_accuracy: 0.7097 - val_auc: 0.7826\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5590 - accuracy: 0.7093 - auc: 0.7836 - val_loss: 0.5603 - val_accuracy: 0.7091 - val_auc: 0.7825\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5500 - accuracy: 0.7161 - auc: 0.7919 - val_loss: 0.5591 - val_accuracy: 0.7118 - val_auc: 0.7832\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5417 - accuracy: 0.7216 - auc: 0.7994 - val_loss: 0.5659 - val_accuracy: 0.7068 - val_auc: 0.7776\n",
      "Epoch 5/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5313 - accuracy: 0.7295 - auc: 0.8084 - val_loss: 0.5706 - val_accuracy: 0.7034 - val_auc: 0.7753\n",
      "Epoch 6/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.5145 - accuracy: 0.7423 - auc: 0.8228\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5147 - accuracy: 0.7421 - auc: 0.8227 - val_loss: 0.5785 - val_accuracy: 0.6964 - val_auc: 0.7679\n",
      "Epoch 7/100\n",
      "196565/196565 [==============================] - 12s 59us/sample - loss: 0.4905 - accuracy: 0.7592 - auc: 0.8415 - val_loss: 0.5955 - val_accuracy: 0.6933 - val_auc: 0.7596\n",
      "Epoch 8/100\n",
      "196352/196565 [============================>.] - ETA: 0s - loss: 0.4630 - accuracy: 0.7783 - auc: 0.8612Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.4630 - accuracy: 0.7783 - auc: 0.8612 - val_loss: 0.6173 - val_accuracy: 0.6863 - val_auc: 0.7514\n",
      "Epoch 00008: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5981 - accuracy: 0.6849 - auc: 0.7492 - val_loss: 0.5591 - val_accuracy: 0.7079 - val_auc: 0.7820\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5583 - accuracy: 0.7113 - auc: 0.7845 - val_loss: 0.5581 - val_accuracy: 0.7073 - val_auc: 0.7829\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5503 - accuracy: 0.7176 - auc: 0.7922 - val_loss: 0.5605 - val_accuracy: 0.7081 - val_auc: 0.7819\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5419 - accuracy: 0.7220 - auc: 0.7994 - val_loss: 0.5637 - val_accuracy: 0.7047 - val_auc: 0.7796\n",
      "Epoch 5/100\n",
      "195840/196565 [============================>.] - ETA: 0s - loss: 0.5309 - accuracy: 0.7306 - auc: 0.8091\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 12s 59us/sample - loss: 0.5310 - accuracy: 0.7305 - auc: 0.8090 - val_loss: 0.5692 - val_accuracy: 0.7029 - val_auc: 0.7741\n",
      "Epoch 6/100\n",
      "196565/196565 [==============================] - 12s 59us/sample - loss: 0.5125 - accuracy: 0.7442 - auc: 0.8247 - val_loss: 0.5785 - val_accuracy: 0.6972 - val_auc: 0.7677\n",
      "Epoch 7/100\n",
      "195584/196565 [============================>.] - ETA: 0s - loss: 0.4876 - accuracy: 0.7611 - auc: 0.8438Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.4877 - accuracy: 0.7611 - auc: 0.8438 - val_loss: 0.5999 - val_accuracy: 0.6903 - val_auc: 0.7601\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196565 samples, validate on 28081 samples\n",
      "Epoch 1/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5981 - accuracy: 0.6854 - auc: 0.7497 - val_loss: 0.5594 - val_accuracy: 0.7049 - val_auc: 0.7816\n",
      "Epoch 2/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5583 - accuracy: 0.7117 - auc: 0.7847 - val_loss: 0.5595 - val_accuracy: 0.7065 - val_auc: 0.7819\n",
      "Epoch 3/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5499 - accuracy: 0.7176 - auc: 0.7925 - val_loss: 0.5595 - val_accuracy: 0.7081 - val_auc: 0.7820\n",
      "Epoch 4/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5418 - accuracy: 0.7230 - auc: 0.7995 - val_loss: 0.5654 - val_accuracy: 0.7062 - val_auc: 0.7790\n",
      "Epoch 5/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5300 - accuracy: 0.7319 - auc: 0.8102 - val_loss: 0.5744 - val_accuracy: 0.7004 - val_auc: 0.7726\n",
      "Epoch 6/100\n",
      "196352/196565 [============================>.] - ETA: 0s - loss: 0.5115 - accuracy: 0.7449 - auc: 0.8252\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.5116 - accuracy: 0.7449 - auc: 0.8252 - val_loss: 0.5791 - val_accuracy: 0.6974 - val_auc: 0.7689\n",
      "Epoch 7/100\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.4859 - accuracy: 0.7626 - auc: 0.8451 - val_loss: 0.5971 - val_accuracy: 0.6898 - val_auc: 0.7585\n",
      "Epoch 8/100\n",
      "195584/196565 [============================>.] - ETA: 0s - loss: 0.4581 - accuracy: 0.7805 - auc: 0.8645Restoring model weights from the end of the best epoch.\n",
      "196565/196565 [==============================] - 12s 60us/sample - loss: 0.4582 - accuracy: 0.7804 - auc: 0.8644 - val_loss: 0.6242 - val_accuracy: 0.6805 - val_auc: 0.7483\n",
      "Epoch 00008: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196566 samples, validate on 28080 samples\n",
      "Epoch 1/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5971 - accuracy: 0.6857 - auc: 0.7505 - val_loss: 0.5670 - val_accuracy: 0.7045 - val_auc: 0.7755\n",
      "Epoch 2/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5579 - accuracy: 0.7105 - auc: 0.7847 - val_loss: 0.5663 - val_accuracy: 0.7049 - val_auc: 0.7775\n",
      "Epoch 3/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5488 - accuracy: 0.7177 - auc: 0.7930 - val_loss: 0.5674 - val_accuracy: 0.7074 - val_auc: 0.7764\n",
      "Epoch 4/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5423 - accuracy: 0.7219 - auc: 0.7990 - val_loss: 0.5717 - val_accuracy: 0.7063 - val_auc: 0.7744\n",
      "Epoch 5/100\n",
      "195840/196566 [============================>.] - ETA: 0s - loss: 0.5308 - accuracy: 0.7304 - auc: 0.8090\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5310 - accuracy: 0.7303 - auc: 0.8089 - val_loss: 0.5765 - val_accuracy: 0.7012 - val_auc: 0.7693\n",
      "Epoch 6/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5135 - accuracy: 0.7420 - auc: 0.8233 - val_loss: 0.5914 - val_accuracy: 0.6969 - val_auc: 0.7628\n",
      "Epoch 7/100\n",
      "195840/196566 [============================>.] - ETA: 0s - loss: 0.4906 - accuracy: 0.7592 - auc: 0.8415Restoring model weights from the end of the best epoch.\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.4905 - accuracy: 0.7593 - auc: 0.8416 - val_loss: 0.6105 - val_accuracy: 0.6896 - val_auc: 0.7561\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "done creating data\n",
      "Train on 196566 samples, validate on 28080 samples\n",
      "Epoch 1/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5987 - accuracy: 0.6848 - auc: 0.7489 - val_loss: 0.5580 - val_accuracy: 0.7105 - val_auc: 0.7836\n",
      "Epoch 2/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5581 - accuracy: 0.7109 - auc: 0.7846 - val_loss: 0.5572 - val_accuracy: 0.7095 - val_auc: 0.7845\n",
      "Epoch 3/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5502 - accuracy: 0.7172 - auc: 0.7920 - val_loss: 0.5598 - val_accuracy: 0.7090 - val_auc: 0.7828\n",
      "Epoch 4/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5420 - accuracy: 0.7222 - auc: 0.7993 - val_loss: 0.5696 - val_accuracy: 0.7072 - val_auc: 0.7798\n",
      "Epoch 5/100\n",
      "195840/196566 [============================>.] - ETA: 0s - loss: 0.5309 - accuracy: 0.7304 - auc: 0.8089\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.001.\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5309 - accuracy: 0.7303 - auc: 0.8089 - val_loss: 0.5674 - val_accuracy: 0.7026 - val_auc: 0.7753\n",
      "Epoch 6/100\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.5141 - accuracy: 0.7426 - auc: 0.8231 - val_loss: 0.5839 - val_accuracy: 0.6971 - val_auc: 0.7691\n",
      "Epoch 7/100\n",
      "195840/196566 [============================>.] - ETA: 0s - loss: 0.4897 - accuracy: 0.7610 - auc: 0.8423Restoring model weights from the end of the best epoch.\n",
      "196566/196566 [==============================] - 12s 60us/sample - loss: 0.4896 - accuracy: 0.7610 - auc: 0.8423 - val_loss: 0.6076 - val_accuracy: 0.6868 - val_auc: 0.7584\n",
      "Epoch 00007: early stopping\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "W = []\n",
    "\n",
    "submission = test_rank[['id']]\n",
    "submission['target'] = 0\n",
    "\n",
    "for run in range(3):\n",
    "\n",
    "    df_indx = stratified_sample_df(train_rank,'target',112323)\n",
    "\n",
    "\n",
    "    kf = model_selection.KFold(n_splits=8,shuffle=True)\n",
    "\n",
    "    for train_index, test_index in kf.split(df_indx):\n",
    "\n",
    "        model.set_weights(init_W)\n",
    "\n",
    "        trainidx = df_indx[train_index]\n",
    "        testidx = df_indx[test_index]\n",
    "\n",
    "        EMBTRAIN = [train_rank.loc[trainidx,emb_features[k]].values for k in range(D)]\n",
    "#         EMBTRAIN+=[train_rank.loc[trainidx,emb_features].values]\n",
    "\n",
    "        EMBTEST = [train_rank.loc[testidx,emb_features[k]].values for k in range(D)]\n",
    "#         EMBTEST+=[train_rank.loc[testidx,emb_features].values]\n",
    "\n",
    "        YTRAIN = train_rank.loc[trainidx,['target_0','target_1']].values\n",
    "        YTEST = train_rank.loc[testidx,['target_0','target_1']].values\n",
    "\n",
    "        print('done creating data')\n",
    "\n",
    "\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor='val_auc', min_delta=0.0001, patience=5,\n",
    "                                     verbose=1, mode='max', baseline=None, restore_best_weights=True)\n",
    "\n",
    "        rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5,\n",
    "                                          patience=3, min_lr=1e-3, mode='max', verbose=1)\n",
    "\n",
    "\n",
    "        model.fit(EMBTRAIN,\n",
    "                  YTRAIN,\n",
    "                  validation_data=(EMBTEST, YTEST),\n",
    "                  verbose=1,\n",
    "                  batch_size=256,\n",
    "                  callbacks=[es, rlr],\n",
    "                  epochs=100,\n",
    "                 )\n",
    "\n",
    "\n",
    "        XTEST = [test_rank[emb_features[k]].values.astype(np.float32) for k in range(D)]\n",
    "#         XTEST+=[test_rank.loc[:,emb_features].values.astype(np.float32)]\n",
    "\n",
    "        ypred_test = model.predict(XTEST, batch_size=100,workers=6)\n",
    "\n",
    "        submission['target'] += ypred_test[:,1]\n",
    "        print()\n",
    "        gc.collect()\n",
    "\n",
    "        W.append(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.target=submission.target/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600000</td>\n",
       "      <td>0.392636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>600001</td>\n",
       "      <td>0.585291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>600002</td>\n",
       "      <td>0.469941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>600003</td>\n",
       "      <td>0.430713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>600004</td>\n",
       "      <td>0.404220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399995</th>\n",
       "      <td>999995</td>\n",
       "      <td>0.861663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399996</th>\n",
       "      <td>999996</td>\n",
       "      <td>0.825350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399997</th>\n",
       "      <td>999997</td>\n",
       "      <td>0.831648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399998</th>\n",
       "      <td>999998</td>\n",
       "      <td>0.633191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399999</th>\n",
       "      <td>999999</td>\n",
       "      <td>0.548729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    target\n",
       "0       600000  0.392636\n",
       "1       600001  0.585291\n",
       "2       600002  0.469941\n",
       "3       600003  0.430713\n",
       "4       600004  0.404220\n",
       "...        ...       ...\n",
       "399995  999995  0.861663\n",
       "399996  999996  0.825350\n",
       "399997  999997  0.831648\n",
       "399998  999998  0.633191\n",
       "399999  999999  0.548729\n",
       "\n",
       "[400000 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
